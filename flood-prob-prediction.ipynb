{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regression with a Flood Prediction Dataset\n> Playground Series - Season 4, Episode 5","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor,BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:41:31.605325Z","iopub.execute_input":"2025-09-16T17:41:31.605566Z","iopub.status.idle":"2025-09-16T17:41:59.351032Z","shell.execute_reply.started":"2025-09-16T17:41:31.605544Z","shell.execute_reply":"2025-09-16T17:41:59.349976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inspecting the DATA","metadata":{}},{"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/playground-series-s4e5/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/playground-series-s4e5/test.csv\")\nsample_sub=pd.read_csv(\"/kaggle/input/playground-series-s4e5/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:41:59.352579Z","iopub.execute_input":"2025-09-16T17:41:59.353306Z","iopub.status.idle":"2025-09-16T17:42:02.884917Z","shell.execute_reply.started":"2025-09-16T17:41:59.353278Z","shell.execute_reply":"2025-09-16T17:42:02.884106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=df_train.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:02.885732Z","iopub.execute_input":"2025-09-16T17:42:02.886126Z","iopub.status.idle":"2025-09-16T17:42:02.973277Z","shell.execute_reply.started":"2025-09-16T17:42:02.886096Z","shell.execute_reply":"2025-09-16T17:42:02.972467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:02.974084Z","iopub.execute_input":"2025-09-16T17:42:02.974335Z","iopub.status.idle":"2025-09-16T17:42:03.00658Z","shell.execute_reply.started":"2025-09-16T17:42:02.974314Z","shell.execute_reply":"2025-09-16T17:42:03.005648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:03.009485Z","iopub.execute_input":"2025-09-16T17:42:03.009831Z","iopub.status.idle":"2025-09-16T17:42:03.791181Z","shell.execute_reply.started":"2025-09-16T17:42:03.009805Z","shell.execute_reply":"2025-09-16T17:42:03.78997Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:03.792175Z","iopub.execute_input":"2025-09-16T17:42:03.792435Z","iopub.status.idle":"2025-09-16T17:42:03.800521Z","shell.execute_reply.started":"2025-09-16T17:42:03.792413Z","shell.execute_reply":"2025-09-16T17:42:03.799526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=df_train.astype(float)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:03.801632Z","iopub.execute_input":"2025-09-16T17:42:03.802019Z","iopub.status.idle":"2025-09-16T17:42:03.891956Z","shell.execute_reply.started":"2025-09-16T17:42:03.801985Z","shell.execute_reply":"2025-09-16T17:42:03.890999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:03.892914Z","iopub.execute_input":"2025-09-16T17:42:03.893159Z","iopub.status.idle":"2025-09-16T17:42:03.995978Z","shell.execute_reply.started":"2025-09-16T17:42:03.893139Z","shell.execute_reply":"2025-09-16T17:42:03.99521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"def simplified_getFeats(df):\n    num_cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement', 'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality', 'Siltation', 'AgriculturalPractices', 'Encroachments', 'IneffectiveDisasterPreparedness', 'DrainageSystems', 'CoastalVulnerability', 'Landslides', 'Watersheds', 'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss', 'InadequatePlanning', 'PoliticalFactors']\n\n    scaler = StandardScaler().fit(df[num_cols])\n    df[num_cols] = scaler.transform(df[num_cols])  # Scale early\n\n    # Basic Stats\n    df['mean'] = df[num_cols].mean(axis=1)\n    df['std'] = df[num_cols].std(axis=1)\n    df['max'] = df[num_cols].max(axis=1)\n    df['min'] = df[num_cols].min(axis=1)\n\n    # Interaction Features (Simplified)\n    df['Climate_Risk'] = df['MonsoonIntensity'] * df['ClimateChange']  \n    df['Infrastructure_Risk'] = df['DamsQuality'] * df['DrainageSystems'] \n\n    return df\n\ndf_train['typ']=0\ndf_test['typ']=1\n# Combine Data\ndf_all = pd.concat([df_train, df_test], axis=0)\ndf_all = simplified_getFeats(df_all)\n\n# Split Back\ndf_train = df_all[df_all['typ'] == 0].drop(['typ'], axis=1)\ndf_test = df_all[df_all['typ'] == 1].drop(['typ'], axis=1)\n\n# Prepare for Model\nX = df_train.drop(['id', 'FloodProbability'], axis=1)\ny = df_train['FloodProbability']\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:03.996739Z","iopub.execute_input":"2025-09-16T17:42:03.996956Z","iopub.status.idle":"2025-09-16T17:42:08.671618Z","shell.execute_reply.started":"2025-09-16T17:42:03.996938Z","shell.execute_reply":"2025-09-16T17:42:08.670327Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for feature in df_train.columns[:-1]:\n    plt.scatter(df_train[feature], df_train['FloodProbability'])\n    plt.xlabel(feature)\n    plt.ylabel('FloodProbability')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:42:08.672882Z","iopub.execute_input":"2025-09-16T17:42:08.673398Z","iopub.status.idle":"2025-09-16T17:43:17.172672Z","shell.execute_reply.started":"2025-09-16T17:42:08.673343Z","shell.execute_reply":"2025-09-16T17:43:17.171278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sample = df_train.sample(frac=0.05)\nsns.pairplot(df_sample)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:43:17.173675Z","iopub.execute_input":"2025-09-16T17:43:17.174157Z","iopub.status.idle":"2025-09-16T17:49:40.639874Z","shell.execute_reply.started":"2025-09-16T17:43:17.174132Z","shell.execute_reply":"2025-09-16T17:49:40.638813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = df_train.corr()\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:49:40.640958Z","iopub.execute_input":"2025-09-16T17:49:40.641292Z","iopub.status.idle":"2025-09-16T17:49:43.598387Z","shell.execute_reply.started":"2025-09-16T17:49:40.641267Z","shell.execute_reply":"2025-09-16T17:49:43.597408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.hist(figsize=(24, 18))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:49:43.599386Z","iopub.execute_input":"2025-09-16T17:49:43.599653Z","iopub.status.idle":"2025-09-16T17:49:49.935996Z","shell.execute_reply.started":"2025-09-16T17:49:43.599631Z","shell.execute_reply":"2025-09-16T17:49:49.934917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=df_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:49:49.939442Z","iopub.execute_input":"2025-09-16T17:49:49.939753Z","iopub.status.idle":"2025-09-16T17:49:55.87382Z","shell.execute_reply.started":"2025-09-16T17:49:49.939731Z","shell.execute_reply":"2025-09-16T17:49:55.872754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysing Various Regression type Models","metadata":{}},{"cell_type":"markdown","source":"### Kfolds = 5","metadata":{}},{"cell_type":"code","source":"models = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(),\n    'SGD Regressor': SGDRegressor(), \n    'Bayesian Ridge Regression': BayesianRidge(),\n    'Bagging Regressor': BaggingRegressor(base_estimator=lgb.LGBMRegressor(verbose=-1)),\n    'XGBoost Regressor': xgb.XGBRegressor(),\n    'LightGBM Regressor': lgb.LGBMRegressor(verbose=-1),\n}","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:49:55.874833Z","iopub.execute_input":"2025-09-16T17:49:55.875081Z","iopub.status.idle":"2025-09-16T17:49:55.880782Z","shell.execute_reply.started":"2025-09-16T17:49:55.875062Z","shell.execute_reply":"2025-09-16T17:49:55.879659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T17:49:55.88205Z","iopub.execute_input":"2025-09-16T17:49:55.88293Z","iopub.status.idle":"2025-09-16T17:49:56.456982Z","shell.execute_reply.started":"2025-09-16T17:49:55.882896Z","shell.execute_reply":"2025-09-16T17:49:56.456088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\nfor name, model in models.items():\n    mse_values = []\n    r2_values = []\n    for train_index, val_index in kf.split(X):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        r2_values.append(r2_score(y_val, y_pred))\n    avg_r2 = np.mean(r2_values)\n    print(f'{name}: R2 = {avg_r2:.4f}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-09-16T17:49:56.45799Z","iopub.execute_input":"2025-09-16T17:49:56.458332Z","iopub.status.idle":"2025-09-16T17:57:49.275715Z","shell.execute_reply.started":"2025-09-16T17:49:56.458302Z","shell.execute_reply":"2025-09-16T17:57:49.274754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a pipeline with standard scaler and a voting regressor\nvoting_reg = make_pipeline(StandardScaler(),\n                           VotingRegressor([\n                               ('XGBoost Regressor', xgb.XGBRegressor()),\n                               ('LightGBM Regressor', lgb.LGBMRegressor(verbose=-1)),\n                               ('Bagging Regressor', BaggingRegressor(base_estimator=lgb.LGBMRegressor(verbose=-1))),\n                               ('Bayesian Ridge Regression', BayesianRidge()),\n                               ('SGD Regressor', SGDRegressor()),\n                               ('Ridge Regression', Ridge()),\n                               ('Linear Regression', LinearRegression())\n                           ]))\n\n# Train the model with cross-validation and evaluate using mean squared error and R2 score\nscores = cross_val_score(voting_reg, X_train, y_train, cv=5, scoring='r2')\nr2 = scores[1].mean()\nprint(f'R2 (CV) = {r2:.4f}')\n\n# Train the final model and make predictions\nvoting_reg.fit(X_train, y_train)\ny_pred = voting_reg.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f'R2 = {r2:.4f}')\n\n# Create a stacking regressor with a ridge regression as the final estimator\nstacking_reg = make_pipeline(StandardScaler(),\n                             StackingRegressor(estimators=[\n                                 ('XGBoost Regressor', xgb.XGBRegressor()),\n                                 ('LightGBM Regressor', lgb.LGBMRegressor()),\n                                 ('Bagging Regressor', BaggingRegressor(base_estimator=LinearRegression())),\n                                 ('Bayesian Ridge Regression', BayesianRidge()),\n                                 ('SGD Regressor', SGDRegressor()),\n                                 ('Ridge Regression', Ridge()),\n                                 ('Linear Regression', LinearRegression())\n                             ], final_estimator=Ridge()))\n\n# Train the model with cross-validation and evaluate using mean squared error and R2 score\nscores = cross_val_score(stacking_reg, X_train, y_train, cv=5, scoring='r2')\nr2 = scores[1].mean()\nprint(f'R2 (CV) = {r2:.4f}')\n\n# Train the final model and make predictions\nstacking_reg.fit(X_train, y_train)\ny_pred = stacking_reg.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f'R2 = {r2:.4f}')","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2025-09-16T18:23:51.417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Voting Regressor**                                                \nR2(CV) : 0.8578                                             \nR2 : 0.8606\n\n> **Stacking Regressor**                                                \nR2(CV) : 0.8649                                     \nR2 : 0.8692","metadata":{}},{"cell_type":"markdown","source":"# Analysing different architechture Neural Networks","metadata":{}},{"cell_type":"markdown","source":"## Dense + Dropout NN\n\n> Train R2: 0.80, Test R2: 0.80","metadata":{}},{"cell_type":"code","source":"def build_model_1():\n    model = Sequential()\n    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n    model.add(Dropout(0.2))\n    model.add(Dense(96, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\nmodel1 = build_model_1()\n\n# 1. Learning Rate Scheduler: ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n\n# 2. Model Checkpoint: Save the best model based on validation loss\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n\n# Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n\n# Model training with History\nhistory = model1.fit(X_train, y_train, \n                    epochs=100, \n                    batch_size=64, \n                    validation_data=(X_test, y_test), \n                    callbacks=[early_stopping, reduce_lr, checkpoint])\n\n# Calculate R2 scores\ntrain_r2 = r2_score(y_train, model1.predict(X_train))\ntest_r2 = r2_score(y_test, model1.predict(X_test))\nprint(f'Train R2: {train_r2:.2f}, Test R2: {test_r2:.2f}')\n\n# Plotting the training and validation loss\nplt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss During Training') \nplt.show()\n\n# Plotting the R2 score during training\nplt.plot(np.arange(len(history.history['loss'])), [r2_score(y_train, model.predict(X_train)) for _ in range(len(history.history['loss']))], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), [r2_score(y_test, model.predict(X_test)) for _ in range(len(history.history['val_loss']))], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('R2 Score')\nplt.legend()\nplt.title('R2 Score During Training') \nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tensorflow as tf\nimport gc\ntorch.cuda.empty_cache()\ntf.keras.backend.clear_session()\ngc.collect()\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_kg_hide-input":true,"execution":{"execution_failed":"2025-09-16T18:23:51.415Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dense + Dropout + BatchNormalization NN\n\n> Train R2: 0.86, Test R2: 0.86","metadata":{}},{"cell_type":"code","source":"def build_model_2():\n    model = Sequential()\n    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(96, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\nmodel2 = build_model_2()\n\n# Advanced Concepts:\n# 1. Learning Rate Scheduler: ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n\n# 2. Model Checkpoint: Save the best model based on validation loss\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n\n# Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Model training with History\nhistory = model2.fit(X_train, y_train, \n                    epochs=100, \n                    batch_size=64, \n                    validation_data=(X_test, y_test), \n                    callbacks=[early_stopping, reduce_lr, checkpoint])\n\n# Calculate R2 scores\ntrain_r2 = r2_score(y_train, model2.predict(X_train))\ntest_r2 = r2_score(y_test, model2.predict(X_test))\nprint(f'Train R2: {train_r2:.2f}, Test R2: {test_r2:.2f}')\n\n# Plotting the training and validation loss\nplt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss During Training') \nplt.show()\n\n# Plotting the R2 score during training\nplt.plot(np.arange(len(history.history['loss'])), [r2_score(y_train, model2.predict(X_train)) for _ in range(len(history.history['loss']))], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), [r2_score(y_test, model2.predict(X_test)) for _ in range(len(history.history['val_loss']))], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('R2 Score')\nplt.legend()\nplt.title('R2 Score During Training') \nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tensorflow as tf\nimport gc\ntorch.cuda.empty_cache()\ntf.keras.backend.clear_session()\ngc.collect()\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_kg_hide-input":true,"execution":{"execution_failed":"2025-09-16T18:23:51.417Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conv1D + Flatten + Dense NN\n> Train R2: 0.83, Test R2: 0.84","metadata":{}},{"cell_type":"code","source":"def build_model_3():\n    model = Sequential()\n    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n    model.add(Flatten()) \n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1)) \n    model.compile(optimizer='adam', loss='mse')\n    return model\n\nmodel3 = build_model_3()\n\n# Advanced Concepts:\n# 1. Learning Rate Scheduler: ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n\n# 2. Model Checkpoint: Save the best model based on validation loss\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n\n# Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Model training with History\nhistory = model3.fit(X_train, y_train, \n                    epochs=100, \n                    batch_size=64, \n                    validation_data=(X_test, y_test), \n                    callbacks=[early_stopping, reduce_lr, checkpoint])\n\n# Calculate R2 scores\ntrain_r2 = r2_score(y_train, model3.predict(X_train))\ntest_r2 = r2_score(y_test, model3.predict(X_test))\nprint(f'Train R2: {train_r2:.2f}, Test R2: {test_r2:.2f}')\n\n# Plotting the training and validation loss\nplt.plot(np.arange(len(history.history['loss'])), history.history['loss'], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), history.history['val_loss'], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss During Training') \nplt.show()\n\n# Plotting the R2 score during training\nplt.plot(np.arange(len(history.history['loss'])), [r2_score(y_train, model3.predict(X_train)) for _ in range(len(history.history['loss']))], label='train')\nplt.plot(np.arange(len(history.history['val_loss'])), [r2_score(y_test, model3.predict(X_test)) for _ in range(len(history.history['val_loss']))], label='validation') \nplt.xlabel('Epoch')\nplt.ylabel('R2 Score')\nplt.legend()\nplt.title('R2 Score During Training') \nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tensorflow as tf\nimport gc\ntorch.cuda.empty_cache()\ntf.keras.backend.clear_session()\ngc.collect()\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_kg_hide-input":true,"execution":{"execution_failed":"2025-09-16T18:23:51.417Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generating the submission using Stacking Regressor","metadata":{}},{"cell_type":"code","source":"df_test=df_test.drop(\"id\", axis=1)\ndf_test=df_test.drop(\"FloodProbability\", axis=1)\ndf_test.head()","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub[\"FloodProbability\"]=stacking_reg.predict(df_test)\nsample_sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_model=xgb.XGBRegressor()\nxgb_model.fit(X_train, y_train)\nsample_sub[\"FloodProbability\"]=xgb_model.predict(df_test)\nsample_sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Stacking Regressor**  Public Score : 0.86663                                   \n> **XGBRegressor**  Public Score : 0.86631","metadata":{}},{"cell_type":"markdown","source":"# Let's Fine-Tune the hyperparameters for XGBRegressor, LGBMRegressor and then blending the predictions...","metadata":{}},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n        'max_depth': trial.suggest_int('max_depth', 3, 9),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n    }\n\n    # Split data into training and validation sets\n    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=trial.number\n    )\n\n    # Create DMatrix for XGBoost\n    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n    dvalid = xgb.DMatrix(X_val_fold, label=y_val_fold)\n\n    model = xgb.train(\n        params, \n        dtrain, \n        evals=[(dvalid, \"validation\")],  \n        early_stopping_rounds=20,       \n        verbose_eval=False               \n    )\n    \n    # Get the best score from the trained model\n    best_score = model.best_score\n\n    # Report the best score as the objective value\n    return best_score  \n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nxgb_best_params = study.best_params\nprint(\"Best parameters:\", xgb_best_params)","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna.visualization as ov\n\n# Plot optimization history\nov.plot_optimization_history(study)\n\n# Plot parameter importance\nov.plot_param_importances(study)\n\n# Plot slice plot (interactive)\nov.plot_slice(study)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# ... (Assuming X_train and y_train are already defined)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n        'max_depth': trial.suggest_int('max_depth', -1, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'objective': 'regression',  \n        'metric': 'rmse',         \n        'verbose': -1,\n    }\n\n    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=trial.number\n    )\n    \n    # Create LightGBM datasets \n    dtrain = lgb.Dataset(X_train_fold, label=y_train_fold)\n    dvalid = lgb.Dataset(X_val_fold, label=y_val_fold)\n\n    # Train with early stopping\n    model = lgb.train(\n        params, \n        dtrain, \n        valid_sets=[dvalid],\n    )\n\n    # Predict and calculate RMSE on validation set\n    y_pred = model.predict(X_val_fold)\n    rmse = mean_squared_error(y_val_fold, y_pred, squared=False)\n\n    # Optuna minimizes the objective, so return the RMSE directly\n    return rmse \n\n# Create study and optimize\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nlgb_best_params = study.best_params\nprint(\"Best parameters:\", lgb_best_params)\n","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna.visualization as ov\n\n# Plot optimization history\nov.plot_optimization_history(study)\n\n# Plot parameter importance\nov.plot_param_importances(study)\n\n# Plot slice plot (interactive)\nov.plot_slice(study)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optuna_xgb_params={'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.010764655250991451, 'subsample': 0.9665734102634232, 'colsample_bytree': 0.6544043152953372, 'gamma': 0.0015546399226326925, 'reg_alpha': 1.2907247281476344e-06, 'reg_lambda': 6.371675646557661e-08}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\noptuna_xgb=xgb.XGBRegressor(**optuna_xgb_params)\noptuna_xgb.fit(X_train, y_train)\nsample_sub[\"FloodProbability_xgb\"]=optuna_xgb.predict(df_test)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\noptuna_lgb=lgb.LGBMRegressor(**lgb_best_params, verbose=-1)\noptuna_lgb.fit(X_train, y_train)\nsample_sub[\"FloodProbability_lgb\"]=optuna_lgb.predict(df_test)","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub[\"FloodProbability\"]=(sample_sub[\"FloodProbability_lgb\"]+sample_sub[\"FloodProbability_xgb\"])/2\nsample_sub.drop([\"FloodProbability_lgb\", \"FloodProbability_xgb\"], axis=1, inplace=True)\nsample_sub.set_index(\"id\", axis=1, inplace=True)\nsample_sub","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\")","metadata":{"execution":{"execution_failed":"2025-09-16T18:23:51.419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Optuna XGBRegressor and LGBMRegressor Blended** Public Score : 0.86679","metadata":{}}]}